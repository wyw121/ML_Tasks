# 常见问题解答 (FAQ)

## 📌 安装相关

### Q1: pip install 很慢怎么办？

**A**: 使用国内镜像源加速：

```powershell
pip install torch pandas numpy scikit-learn matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---

### Q2: 提示"pip不是内部或外部命令"

**A**: Python没有正确安装或未添加到PATH。解决方法：

1. 重新安装Python，勾选"Add Python to PATH"
2. 或使用完整路径：
   ```powershell
   python -m pip install torch pandas numpy scikit-learn matplotlib
   ```

---

### Q3: torch安装失败

**A**: PyTorch安装可能需要特殊处理：

```powershell
# CPU版本（推荐新手使用）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

---

## 📌 运行相关

### Q4: "FileNotFoundError: used_car/used_car_train.csv"

**A**: 检查以下几点：

1. 确保在正确的文件夹运行（包含 train_model.py 的文件夹）
2. 确认 `used_car` 文件夹存在
3. 检查 CSV 文件名是否正确

使用PowerShell确认当前路径：
```powershell
pwd  # 查看当前路径
ls   # 列出当前文件夹内容
```

---

### Q5: 训练很慢，一直卡在某个地方

**A**: 这是正常的！可能的原因：

1. **第一个epoch最慢**：PyTorch在初始化，耐心等待
2. **电脑配置**：CPU训练确实比GPU慢
3. **数据量**：40000条数据需要时间

**加速建议**：
- 减少 `NUM_EPOCHS` 到 50
- 增大 `BATCH_SIZE` 到 128
- 减少网络层数

---

### Q6: "CUDA out of memory" 错误

**A**: 显存不足（如果你有GPU）。解决方法：

1. 减小 `BATCH_SIZE`：64 → 32 → 16
2. 减少网络层数
3. 或强制使用CPU：
   ```python
   device = torch.device('cpu')
   ```

---

## 📌 结果相关

### Q7: 损失（Loss）很大，是不是失败了？

**A**: 不一定！关注以下几点：

- **损失在下降吗**？下降就是好的
- **验证损失也在下降吗**？说明模型在学习
- **损失的绝对值**取决于数据范围，价格从几百到几万，损失几千很正常

**判断标准**：
- ✅ 训练损失和验证损失都在下降
- ✅ 两者差距不大（比如都在2000左右）
- ❌ 训练损失下降但验证损失上升（过拟合）

---

### Q8: 训练损失很小，验证损失很大

**A**: 典型的**过拟合**！模型死记硬背训练数据。

**解决方法**：
1. 减少训练轮数（用 Early Stopping）
2. 减少网络复杂度（更少的层或神经元）
3. 增大 `PATIENCE` 值，更早停止

---

### Q9: 损失不下降，一直在某个值

**A**: 可能的原因：

1. **学习率太小**：改为 `lr=0.01`
2. **学习率太大**：改为 `lr=0.0001`
3. **数据问题**：检查是否正确标准化
4. **模型太简单**：增加网络层数

**调试步骤**：
```python
# 1. 先检查数据
print(X_train.mean(), X_train.std())  # 应该接近0和1

# 2. 打印每个batch的损失
print(f"Batch loss: {loss.item()}")

# 3. 尝试不同学习率
```

---

### Q10: submission.csv 的价格都是负数或很奇怪

**A**: 可能的问题：

1. **标准化问题**：确保测试集使用训练集的 scaler
2. **模型未训练好**：损失还没收敛就停止了
3. **数据问题**：检查是否有缺失值未处理

**检查方法**：
```python
# 查看预测值的统计信息
print(f"预测值范围: {predictions.min()} ~ {predictions.max()}")
print(f"预测值均值: {predictions.mean()}")

# 与训练集价格对比
print(f"训练集价格范围: {y_train.min()} ~ {y_train.max()}")
```

---

## 📌 理论相关

### Q11: 什么是 epoch、batch、iteration？

**A**: 通俗解释：

- **Epoch**：把所有训练数据看一遍
- **Batch**：一次处理的数据数量（如64个样本）
- **Iteration**：一次参数更新

**例子**：
- 训练集：40000个样本
- Batch Size：64
- 1个epoch = 40000/64 ≈ 625 iterations

---

### Q12: 为什么要分训练集和验证集？

**A**: 

**训练集**：模型从这里学习（学生做练习题）  
**验证集**：模型在这里测试（学生做模拟考）  
**测试集**：最终评估（正式考试）

**目的**：
- 防止模型"死记硬背"训练数据
- 及时发现过拟合
- 选择最佳模型

---

### Q13: ReLU 激活函数是什么？

**A**: 非常简单的函数：

```python
ReLU(x) = max(0, x)
```

**作用**：
- 如果输入是负数，输出0
- 如果输入是正数，输出原值

**为什么用它**：
- 计算快
- 避免梯度消失
- 实践中效果好

---

### Q14: Adam 优化器和 SGD 有什么区别？

**A**: 

| 特性 | SGD | Adam |
|-----|-----|------|
| 学习率 | 固定 | 自适应调整 |
| 速度 | 慢 | 快 |
| 稳定性 | 需要调参 | 默认值就很好 |
| 适用性 | 简单问题 | 大多数问题 |

**李宏毅老师建议**：新手用 Adam！

---

### Q15: 为什么要标准化数据？

**A**: 

**问题**：特征范围差异大
- 功率：50-300
- 价格：500-50000

**后果**：
- 梯度下降会被大数值主导
- 训练不稳定
- 收敛慢

**解决**：标准化后都变成均值0、标准差1
```python
标准化后的值 = (原值 - 均值) / 标准差
```

---

## 📌 调参相关

### Q16: 应该先调哪个参数？

**A**: 推荐顺序：

1. **先用默认值运行**：确保代码能跑
2. **调学习率**：对结果影响最大
3. **调 Batch Size**：影响训练速度
4. **调网络结构**：最后考虑
5. **每次只改一个参数**！

---

### Q17: 怎么判断参数调得好不好？

**A**: 看训练曲线 (training_curve.png)：

**好的曲线**：
```
Loss
 ↑
 |  \
 |   \___  训练损失
 |    \___ 验证损失
 |        \___
 |            ------  收敛
 └──────────────────→ Epoch
```

**坏的曲线（过拟合）**：
```
Loss
 ↑
 |   训练\___________  继续下降
 |   
 |   验证    /-------  开始上升！
 |         /
 └──────────────────→ Epoch
```

---

### Q18: 训练多少个 epoch 合适？

**A**: 

- **不要固定轮数**！使用 Early Stopping
- 让模型自己决定何时停止
- 一般50-200个epoch足够

**设置建议**：
```python
NUM_EPOCHS = 500     # 最大值
PATIENCE = 20        # 20轮不改善就停
```

实际可能在100轮左右就停止了。

---

## 📌 代码相关

### Q19: 代码里的 torch.no_grad() 是什么意思？

**A**: 

```python
with torch.no_grad():
    # 这里的代码不计算梯度
```

**用途**：
- 验证/测试时不需要计算梯度
- 节省内存
- 加快速度

**对比**：
- 训练时：需要梯度 → 不用 no_grad
- 验证时：不需要梯度 → 用 no_grad

---

### Q20: model.train() 和 model.eval() 有什么区别？

**A**: 

```python
model.train()  # 训练模式
# - 启用 Dropout（如果有）
# - 启用 Batch Normalization 更新（如果有）

model.eval()   # 评估模式
# - 关闭 Dropout
# - 使用固定的 Batch Normalization 参数
```

**虽然我们的模型没用这些**，但养成好习惯很重要！

---

## 📌 进阶问题

### Q21: 怎么知道模型是否学到了有用的东西？

**A**: 

1. **对比**：预测值和真实值的分布
2. **可视化**：画散点图（预测 vs 真实）
3. **评估指标**：
   - MAE (平均绝对误差)
   - RMSE (均方根误差)
   - R² (决定系数)

**代码示例**：
```python
from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(y_valid, predictions)
r2 = r2_score(y_valid, predictions)
print(f"MAE: {mae}, R²: {r2}")
```

---

### Q22: 能否用更简单的方法（不用深度学习）？

**A**: 当然可以！

**传统机器学习方法**：
- 线性回归
- 随机森林
- XGBoost

**为什么还用深度学习**？
- 学习目的：理解神经网络
- 自动特征学习：不需要手工设计特征
- 可扩展性：数据多时表现更好

**建议**：先学会深度学习，再对比传统方法。

---

### Q23: 如何改进模型性能？

**A**: 进阶技巧：

1. **特征工程**：
   ```python
   # 创建车龄特征
   data['car_age'] = 2025 - data['regDate']
   ```

2. **数据清洗**：
   - 删除异常值
   - 处理离群点

3. **模型集成**：
   - 训练多个模型
   - 取预测的平均值

4. **超参数搜索**：
   - Grid Search
   - Random Search

---

## 📌 其他问题

### Q24: 训练好的模型能保存吗？

**A**: 已经保存了！

```python
# 代码里已经有了
torch.save(model.state_dict(), 'best_model.pth')

# 加载模型
model.load_state_dict(torch.load('best_model.pth'))
```

---

### Q25: 这个作业能得高分吗？

**A**: 能！因为：

✅ 代码完整且有详细注释  
✅ 使用了课程里的核心概念  
✅ 有训练曲线图  
✅ 有理论解释文档  
✅ 结果合理

**加分项**：
- 在 README 里写实验对比
- 分析不同参数的影响
- 画更多可视化图表

---

## 🎓 学习建议

遇到问题时：

1. **先看报错信息**：最后一行通常是关键
2. **Google搜索**：复制错误信息搜索
3. **对照代码**：检查是否有拼写错误
4. **简化问题**：先运行最简单的版本
5. **打印调试**：多用 `print()` 查看中间结果

---

**记住**：所有高手都是从遇到问题、解决问题开始的！💪

每个问题都是学习的机会，加油！🚀
