# 二手车价格预测 - 深度学习作业详解

## 📚 学习目标
本文档将帮助你理解如何将李宏毅老师讲的深度学习基础知识应用到实际问题中。

---

## 1️⃣ 问题理解

### 什么是回归问题？
- **分类问题**：预测类别（猫或狗）
- **回归问题**：预测数值（价格、温度等）

我们的任务：根据二手车的各种特征（品牌、车型、里程等），预测它的**价格**。

### 数据集说明
- **训练集** (`used_car_train.csv`)：40,001条数据，包含price（价格）
- **测试集** (`used_car_test.csv`)：4,001条数据，需要我们预测price

---

## 2️⃣ 李宏毅老师教的核心概念回顾

### 2.1 梯度下降 (Gradient Descent)

**核心思想**：想象你站在山上，想走到最低点（损失函数最小值）

```
参数更新公式：θ = θ - η * ∇L
```

- **θ (theta)**：模型参数（权重和偏置）
- **η (eta)**：学习率，控制每步走多远
- **∇L**：损失函数的梯度，告诉你该往哪个方向走

**问题**：如果每次用所有数据计算梯度，计算量太大！

---

### 2.2 Mini-batch 梯度下降

**解决方案**：把数据分成小批次（batch）

- **Batch Size = 32**：每次随机抽32个样本计算梯度
- **Epoch**：把所有训练数据都看过一遍

**优点**：
- 计算快（不用每次用全部数据）
- 有随机性，帮助跳出局部最优解

---

### 2.3 优化器 (Optimizer)

#### (1) SGD with Momentum（动量）
**问题**：普通梯度下降会震荡，走得慢

**解决**：加入"惯性"，记住之前的方向
```
v = λv - η∇L     # 累积过去的梯度
θ = θ + v         # 更新参数
```

**比喻**：像滚雪球，越滚越快

#### (2) Adaptive Learning Rate（自适应学习率）

**Adam优化器**（最常用）：
- 自动调整每个参数的学习率
- 对不同参数用不同步长
- 结合了Momentum和RMSprop的优点

---

### 2.4 损失函数 (Loss Function)

**回归问题常用**：均方误差 (MSE - Mean Squared Error)

```
Loss = (1/N) Σ(预测值 - 真实值)²
```

**为什么平方**？
- 惩罚大误差（误差10比误差1严重100倍）
- 数学上方便求导

---

### 2.5 避免过拟合 (Overfitting)

**现象**：模型在训练集表现好，测试集表现差

**李宏毅老师讲的方法**：
1. **Validation Set**（验证集）：从训练集分出一部分，监控过拟合
2. **Early Stopping**：验证集损失不再下降时停止训练

---

## 3️⃣ 代码实现步骤详解

### 步骤1：数据预处理

#### 为什么要标准化？
不同特征的范围差异很大：
- `power`（功率）：可能是 50-300
- `price`（价格）：可能是 500-50000

**问题**：梯度下降会偏向数值大的特征

**解决**：标准化 (Standardization)
```python
X = (X - mean) / std  # 转换为均值0，标准差1
```

#### 处理缺失值
数据中有 `-` 符号表示缺失，需要：
1. 转换为数值（如NaN）
2. 填充（用中位数或众数）

---

### 步骤2：构建神经网络

```python
模型结构：
输入层 (30个特征)
    ↓
隐藏层1 (128个神经元) + ReLU激活
    ↓
隐藏层2 (64个神经元) + ReLU激活
    ↓
隐藏层3 (32个神经元) + ReLU激活
    ↓
输出层 (1个神经元，预测价格)
```

**为什么用ReLU激活函数**？
- 简单：max(0, x)
- 避免梯度消失
- 训练快

---

### 步骤3：训练过程

```python
for epoch in range(总轮数):
    for batch in 数据分批:
        # 1. 前向传播：计算预测值
        预测值 = 模型(输入数据)
        
        # 2. 计算损失
        损失 = MSE(预测值, 真实值)
        
        # 3. 反向传播：计算梯度
        损失.backward()
        
        # 4. 更新参数（Adam优化器自动调整学习率）
        优化器.step()
```

**重要概念对应**：
- `optimizer.step()` → 执行梯度下降
- `loss.backward()` → 计算梯度 ∇L
- `Adam` → 自适应学习率优化器

---

### 步骤4：验证和Early Stopping

```python
# 每个epoch结束后
验证损失 = 在验证集上计算损失

if 验证损失 < 最佳损失:
    保存模型  # 这是目前最好的模型
    耐心计数器 = 0
else:
    耐心计数器 += 1
    
if 耐心计数器 > 耐心阈值:
    停止训练  # 避免过拟合
```

---

## 4️⃣ 超参数选择指南

| 超参数 | 推荐值 | 李宏毅老师的建议 |
|--------|--------|------------------|
| **学习率** | 0.001 | Adam已自动调整，用默认值 |
| **Batch Size** | 32-128 | 太小训练慢，太大泛化差 |
| **Epoch** | 100-500 | 用Early Stopping自动停止 |
| **隐藏层神经元数** | 128→64→32 | 逐层递减，先宽后窄 |

---

## 5️⃣ 常见问题与解决

### Q1: 损失不下降？
- ✅ 检查学习率（太大或太小）
- ✅ 检查数据是否标准化
- ✅ 尝试更深的网络

### Q2: 训练集准确但测试集差？
- ✅ 过拟合！减少训练轮数
- ✅ 使用Early Stopping

### Q3: 训练很慢？
- ✅ 增大Batch Size
- ✅ 减少网络层数

---

## 6️⃣ 代码与李宏毅课程的对应关系

| 代码部分 | 对应课程 | 视频位置 |
|----------|----------|----------|
| `torch.optim.Adam` | Adaptive Learning Rate | optimizer ppt |
| `DataLoader(batch_size=32)` | Tips for Training: Batch | Batch and Momentum |
| `nn.ReLU()` | Activation Function | Deep Learning basics |
| `MSELoss()` | Loss Function | Regression HW1 |
| `model.train() / model.eval()` | Training Process | General Guidance |

---

## 7️⃣ 实验建议

1. **先运行基础版本**：理解整个流程
2. **调整超参数**：
   - 改变学习率：0.0001, 0.001, 0.01
   - 改变Batch Size：16, 32, 64, 128
3. **观察训练曲线**：
   - 训练损失和验证损失的变化
   - 判断是否过拟合

---

## 8️⃣ 总结

你已经学到的知识**完全足够**解决这个问题：
- ✅ 梯度下降 → PyTorch自动完成
- ✅ Batch训练 → DataLoader实现
- ✅ Adam优化器 → 自适应学习率
- ✅ Early Stopping → 防止过拟合

**关键**：理解每行代码背后的原理，而不是死记硬背！

---

## 📖 推荐学习路径

1. **运行代码**，看到结果
2. **阅读注释**，理解每一步
3. **修改参数**，观察影响
4. **对照课程**，巩固理论

加油！深度学习的实战就是这样一步步积累的！🚀
