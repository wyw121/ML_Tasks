# 实验指南 - 深度学习超参数调优

## 🎯 实验目的

通过调整超参数，理解它们对模型训练的影响，这是李宏毅老师课程的核心实践内容。

---

## 📊 实验1：学习率的影响

### 理论回顾
学习率 (Learning Rate) 控制梯度下降的步长：
- **太大**：跳过最优解，损失震荡
- **太小**：训练太慢，可能陷入局部最优

### 实验步骤

在 `train_model.py` 第 176 行修改：

```python
# 实验A：小学习率
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# 实验B：正常学习率（默认）
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 实验C：大学习率
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
```

### 预期结果

| 学习率 | 训练速度 | 最终损失 | 说明 |
|--------|---------|---------|------|
| 0.0001 | 很慢 | 可能较高 | 收敛慢，需要更多轮数 |
| 0.001 | 适中 | 较好 | **推荐值** |
| 0.01 | 很快 | 可能不稳定 | 可能震荡或发散 |

### 观察要点
- 看训练曲线是否平滑
- 验证损失是否持续下降
- 训练需要多少个 epoch 收敛

---

## 📊 实验2：Batch Size 的影响

### 理论回顾
Batch Size 决定每次更新参数时使用多少样本：
- **小批次**：更新频繁，有噪声，泛化好
- **大批次**：更新稳定，训练快，但可能过拟合

### 实验步骤

在 `train_model.py` 第 146 行修改：

```python
# 实验A：小批次
BATCH_SIZE = 16

# 实验B：中等批次（默认）
BATCH_SIZE = 64

# 实验C：大批次
BATCH_SIZE = 256
```

### 预期结果

| Batch Size | 每Epoch时间 | 泛化能力 | 内存占用 |
|-----------|------------|---------|---------|
| 16 | 慢 | 好 | 低 |
| 64 | 适中 | 较好 | 中 |
| 256 | 快 | 可能差 | 高 |

### 观察要点
- 训练速度差异
- 验证集和训练集损失的差距（泛化能力）
- 训练曲线的平滑程度

---

## 📊 实验3：网络深度的影响

### 理论回顾
网络层数影响模型的表达能力：
- **太浅**：欠拟合，学不到复杂模式
- **太深**：过拟合，参数多，训练慢

### 实验步骤

修改 `train_model.py` 第 163-180 行的模型定义：

#### 方案A：浅层网络（2层）
```python
self.layers = nn.Sequential(
    nn.Linear(input_dim, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)
```

#### 方案B：中等网络（3层，默认）
```python
self.layers = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1)
)
```

#### 方案C：深层网络（5层）
```python
self.layers = nn.Sequential(
    nn.Linear(input_dim, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 16),
    nn.ReLU(),
    nn.Linear(16, 1)
)
```

### 预期结果

| 网络深度 | 参数量 | 训练速度 | 性能 |
|---------|-------|---------|------|
| 浅层（2层） | 少 | 快 | 可能欠拟合 |
| 中等（3层） | 适中 | 适中 | **平衡** |
| 深层（5层） | 多 | 慢 | 可能过拟合 |

---

## 📊 实验4：Early Stopping 的作用

### 理论回顾
Early Stopping 防止过拟合：
- 监控验证集损失
- 如果不再改善，提前停止训练

### 实验步骤

在 `train_model.py` 第 190 行修改：

```python
# 实验A：不使用 Early Stopping
PATIENCE = 999999  # 实际上不会触发

# 实验B：激进的 Early Stopping
PATIENCE = 10

# 实验C：温和的 Early Stopping（默认）
PATIENCE = 20
```

### 观察要点
- 比较训练损失和验证损失
- 验证损失开始上升的时机
- 最佳模型出现在哪个 epoch

---

## 📊 实验5：优化器对比

### 理论回顾
不同优化器有不同特点：
- **SGD**：基础，需要手动调学习率
- **SGD + Momentum**：加速收敛
- **Adam**：自适应学习率，最常用

### 实验步骤

在 `train_model.py` 第 176 行替换优化器：

```python
# 实验A：普通 SGD
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# 实验B：SGD + Momentum
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# 实验C：Adam（默认）
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

### 预期结果

| 优化器 | 收敛速度 | 稳定性 | 最终性能 |
|-------|---------|-------|---------|
| SGD | 慢 | 不稳定 | 一般 |
| SGD+Momentum | 较快 | 较稳定 | 较好 |
| Adam | **快** | **稳定** | **好** |

---

## 📝 实验记录表格

建议使用以下表格记录实验结果：

| 实验编号 | 学习率 | Batch Size | 网络层数 | 优化器 | 最佳验证损失 | 收敛Epoch | 备注 |
|---------|-------|-----------|---------|-------|------------|----------|------|
| 1 | 0.001 | 64 | 3 | Adam | ? | ? | 基线 |
| 2 | 0.0001 | 64 | 3 | Adam | ? | ? | 小学习率 |
| 3 | 0.001 | 32 | 3 | Adam | ? | ? | 小批次 |
| ... | ... | ... | ... | ... | ... | ... | ... |

---

## 🎓 学习建议

### 第一阶段：单变量实验
每次只改变**一个**超参数，其他保持不变，这样能清楚看到单个参数的影响。

### 第二阶段：组合实验
找到最佳的参数组合，例如：
- 学习率 0.001 + Batch Size 64
- 学习率 0.0001 + Batch Size 32

### 第三阶段：理解原理
对照李宏毅老师的课程，理解为什么某些参数组合效果更好。

---

## 💡 调参技巧（李宏毅老师的建议）

1. **先用默认值**：Adam + lr=0.001 是很好的起点
2. **观察训练曲线**：
   - 损失不下降 → 增大学习率或检查数据
   - 损失震荡 → 减小学习率
   - 训练/验证差距大 → 过拟合，用 Early Stopping
3. **Grid Search**：系统地尝试参数组合
4. **从粗到细**：先大范围搜索，再精细调整

---

## ⚠️ 常见错误

### 错误1：同时改多个参数
❌ 不知道哪个参数起了作用  
✅ 每次只改一个参数

### 错误2：没有保存实验记录
❌ 忘记哪个参数组合最好  
✅ 用表格记录每次实验

### 错误3：只看训练损失
❌ 忽略过拟合  
✅ 同时关注验证损失

---

## 📚 对应课程内容

| 实验 | 对应课程 | 关键概念 |
|-----|---------|---------|
| 学习率 | Adaptive Learning Rate | η 的选择 |
| Batch Size | Batch and Momentum | Mini-batch 训练 |
| 优化器 | Optimizer | SGD vs Adam |
| Early Stopping | Overfitting | 验证集的作用 |

---

## 🚀 进阶挑战

完成基础实验后，可以尝试：

1. **特征工程**：
   - 创建车龄特征
   - 对类别特征做 One-hot 编码

2. **数据增强**：
   - 处理异常值
   - 特征归一化方法对比

3. **模型集成**：
   - 训练多个模型
   - 取平均作为最终预测

---

**祝实验顺利！** 🎉

记住：调参是个实践过程，多试多想，对照理论，你会越来越熟练！
