# 📚 完整学习资料索引

亲爱的同学，欢迎来到深度学习实战！这里为你准备了完整的学习资料。

---

## 🎯 你现在拥有的所有文件

### 📘 理论学习文档（按顺序阅读）

1. **快速开始指南.md** ⭐ 从这里开始！
   - 5分钟快速上手
   - 安装和运行指南
   - 适合完全新手

2. **深度学习作业详解.md** ⭐⭐ 核心理论
   - 李宏毅老师课程回顾
   - 梯度下降、Adam、Batch等概念
   - 代码与课程的对应关系

3. **README.md**
   - 项目概览
   - 文件结构说明
   - 快速参考

4. **常见问题解答.md**
   - 遇到问题先来这里查
   - 25个常见问题详解
   - 包括安装、运行、调试

5. **实验指南.md**
   - 5个动手实验
   - 超参数调优教程
   - 适合已经运行成功的同学

---

### 💻 代码文件

1. **train_model.py** ⭐⭐⭐ 核心训练代码
   - 400行完整注释
   - 13个步骤详细说明
   - 包含所有李宏毅课程要点

2. **predict.py**
   - 简化的预测脚本
   - 使用训练好的模型
   - 适合理解推理过程

---

### 📊 数据文件

- `used_car/used_car_train.csv` - 训练数据（40,001条）
- `used_car/used_car_test.csv` - 测试数据（4,001条）

---

## 🗺️ 学习路线图（建议5天计划）

### 第1天：快速启动 ⚡
**目标**：成功运行代码，看到结果

```
1. 阅读《快速开始指南.md》（15分钟）
2. 安装依赖包（10分钟）
3. 运行 train_model.py（20分钟）
4. 查看生成的文件：
   - submission.csv
   - training_curve.png
   - best_model.pth

✅ 完成标志：看到"训练完成"的输出
```

**如果遇到问题**：查看《常见问题解答.md》的Q1-Q6

---

### 第2天：理解理论 📖
**目标**：理解深度学习的基本概念

```
1. 阅读《深度学习作业详解.md》（60分钟）
   重点：
   - 2.1 梯度下降
   - 2.2 Mini-batch
   - 2.3 Adam优化器
   - 2.4 损失函数

2. 对照李宏毅老师的视频（选看）
   - Gradient Descent 部分
   - Optimizer 部分

3. 画图理解概念：
   - 在纸上画梯度下降的过程
   - 理解损失函数如何下降

✅ 完成标志：能用自己的话解释梯度下降
```

**作业**：回答以下问题
- 什么是学习率？太大或太小会怎样？
- 为什么要用 Mini-batch 而不是全部数据？
- Adam 比 SGD 好在哪里？

---

### 第3天：阅读代码 💻
**目标**：理解每一行代码的作用

```
1. 打开 train_model.py，逐段阅读（90分钟）
   
   重点关注：
   - 第七步：DataLoader（Mini-batch实现）
   - 第八步：网络结构
   - 第九步：优化器定义
   - 第十步：训练循环

2. 对照代码和理论文档
   找到以下对应关系：
   - loss.backward() ← 计算梯度
   - optimizer.step() ← 梯度下降
   - DataLoader ← Mini-batch
   - Adam ← 自适应学习率

3. 在代码中添加打印语句
   ```python
   print(f"当前损失: {loss.item()}")
   print(f"批次大小: {batch_features.shape}")
   ```

✅ 完成标志：理解训练循环的4个步骤
```

**练习**：
- 找出代码中实现 Early Stopping 的部分
- 找出保存最佳模型的代码行
- 理解为什么验证时用 `torch.no_grad()`

---

### 第4天：动手实验 🧪
**目标**：通过实验理解超参数的作用

```
1. 阅读《实验指南.md》（30分钟）

2. 完成实验1：学习率影响（60分钟）
   - 运行 lr=0.0001
   - 运行 lr=0.001（默认）
   - 运行 lr=0.01
   - 对比 training_curve.png

3. 完成实验2：Batch Size 影响（60分钟）
   - 运行 batch_size=16
   - 运行 batch_size=64（默认）
   - 运行 batch_size=128
   - 记录训练时间和效果

4. 填写实验记录表格

✅ 完成标志：完成至少2个对比实验
```

**记录模板**：
```
实验1：学习率0.0001
- 最佳验证损失：____
- 收敛轮数：____
- 观察：________

实验2：学习率0.001
- 最佳验证损失：____
- 收敛轮数：____
- 观察：________
```

---

### 第5天：总结提升 📝
**目标**：形成完整的知识体系

```
1. 整理学习笔记（60分钟）
   写一份总结，包括：
   - 深度学习的基本流程
   - 每个超参数的作用
   - 遇到的问题和解决方法
   - 与李宏毅课程的联系

2. 尝试改进模型（选做，90分钟）
   - 特征工程：创建车龄特征
   - 调整网络结构
   - 尝试不同优化器

3. 准备作业报告
   包含：
   - 代码说明
   - 实验结果
   - 训练曲线图
   - 心得体会

✅ 完成标志：写完总结文档


---

## 📋 知识点检查清单

完成学习后，你应该能够：

### 基础概念 ✓
- [ ] 解释什么是梯度下降
- [ ] 说明学习率的作用
- [ ] 理解 Batch Size 的含义
- [ ] 知道什么是 Epoch
- [ ] 理解损失函数的意义

### 优化方法 ✓
- [ ] 知道 SGD 和 Adam 的区别
- [ ] 理解自适应学习率的优势
- [ ] 知道 Momentum 的作用

### 防止过拟合 ✓
- [ ] 理解训练集/验证集的区别
- [ ] 知道什么是过拟合
- [ ] 会使用 Early Stopping

### 代码实现 ✓
- [ ] 会创建 DataLoader
- [ ] 会定义神经网络
- [ ] 会写训练循环
- [ ] 会保存和加载模型

### 调参技巧 ✓
- [ ] 知道从哪些参数开始调
- [ ] 会观察训练曲线
- [ ] 能判断过拟合
- [ ] 知道如何改进模型

---

## 🎯 李宏毅课程对照表

| 学习内容 | 对应课程章节 | 建议观看 |
|---------|------------|---------|
| 梯度下降 | General Guidance | ⭐⭐⭐ |
| Mini-batch | Batch and Momentum | ⭐⭐⭐ |
| Adam优化器 | Adaptive Learning Rate | ⭐⭐⭐ |
| 损失函数 | Loss Function | ⭐⭐ |
| 过拟合 | Guideline of ML | ⭐⭐ |
| 激活函数 | Introduction of DL | ⭐ |

---

## 💡 学习小贴士

### 高效学习方法
1. **理论先行**：先理解概念再看代码
2. **动手实践**：必须自己运行代码
3. **对比学习**：改参数看效果
4. **及时记录**：写下观察和思考
5. **循序渐进**：不要跳步

### 避免的误区
❌ 只看代码不理解原理  
❌ 只看理论不动手实践  
❌ 遇到问题立刻放弃  
❌ 同时改多个参数  
❌ 不记录实验结果  

### 遇到困难时
1. **查FAQ**：《常见问题解答.md》
2. **Google**：搜索错误信息
3. **简化**：先跑通最简单版本
4. **打印**：用 print() 调试
5. **休息**：换个时间再试

---

## 📊 学习进度追踪

### Week 1: 入门阶段
- [ ] Day 1: 成功运行代码
- [ ] Day 2: 理解基本概念
- [ ] Day 3: 读懂代码逻辑
- [ ] Day 4: 完成对比实验
- [ ] Day 5: 写出总结报告

### Week 2: 提升阶段（选做）
- [ ] 尝试特征工程
- [ ] 对比不同网络结构
- [ ] 学习更多优化技巧
- [ ] 研究其他深度学习模型

---

## 🎓 作业提交建议

### 必须包含
1. **代码文件**
   - train_model.py（可添加你的修改）
   - 简要说明如何运行

2. **结果文件**
   - submission.csv
   - training_curve.png

3. **实验报告**（Word/PDF）
   - 问题描述
   - 方法说明（神经网络结构、超参数）
   - 实验结果（训练曲线、损失值）
   - 心得体会

### 加分项
- 对比不同超参数的实验
- 详细的训练过程分析
- 与李宏毅课程的对应说明
- 遇到的问题和解决方法
- 更多可视化图表

---

## 🔗 快速导航

遇到不同问题，查看不同文档：

| 问题类型 | 查看文档 |
|---------|---------|
| 不知道如何开始 | 快速开始指南.md |
| 不理解概念 | 深度学习作业详解.md |
| 代码报错 | 常见问题解答.md (Q1-Q10) |
| 结果不对 | 常见问题解答.md (Q7-Q10) |
| 想要调参 | 实验指南.md |
| 想要改进 | 常见问题解答.md (Q21-Q23) |

---

## ✨ 最后的鼓励

深度学习看起来很复杂，但你已经有了：
- ✅ 详细的理论讲解
- ✅ 完整的代码实现
- ✅ 系统的学习路线
- ✅ 常见问题解答
- ✅ 动手实验指导

**你需要的只是**：
1. 耐心 - 一步一步来
2. 动手 - 必须自己运行
3. 思考 - 理解背后原理

**记住**：
- 李宏毅老师的课程已经把深度学习讲得很简单了
- 这个作业用的都是最基础的方法
- 你学过的知识完全够用
- 遇到问题很正常，解决它就是进步

---

## 📞 学习检查点

完成每个阶段后，问自己：

**Day 1 后**：代码能跑通了吗？  
**Day 2 后**：能解释梯度下降吗？  
**Day 3 后**：理解训练循环吗？  
**Day 4 后**：知道参数怎么调了吗？  
**Day 5 后**：能向别人讲清楚吗？  

如果答案都是"是"，恭喜你！你已经掌握了深度学习的基础！🎉

---

**开始你的深度学习之旅吧！** 🚀

第一步：打开《快速开始指南.md》

加油，你一定可以的！💪
