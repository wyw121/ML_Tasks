# 深度学习常用函数详解

## 📚 目录
1. [数据处理函数](#数据处理函数)
2. [PyTorch基础](#pytorch基础)
3. [神经网络组件](#神经网络组件)
4. [训练相关](#训练相关)
5. [在代码中的应用](#在代码中的应用)

---

## 🔧 数据处理函数

### Pandas 函数
```python
pd.read_csv('文件名')           # 读取CSV文件
df.replace('-', np.nan)        # 替换值（把'-'替换成NaN）
df.columns                     # 获取列名
df.values                      # 转换成numpy数组
df.astype(float)              # 转换数据类型
```

### NumPy 函数
```python
np.nan                         # 表示缺失值
np.nan_to_num(array, 0)       # 把NaN替换成指定值（这里是0）
np.clip(array, min, max)      # 限制数组值在范围内
array.min(), array.max()      # 获取最小值、最大值
array.reshape(-1, 1)          # 改变数组形状
array.flatten()               # 展平数组
```

### Sklearn 预处理
```python
StandardScaler()              # 标准化器（让数据均值为0，标准差为1）
scaler.fit_transform(X)       # 计算参数并转换数据
scaler.transform(X)           # 用已有参数转换数据
scaler.inverse_transform(X)   # 反向转换（恢复原始数据）

train_test_split()            # 分割数据集
# 参数说明：
# - test_size=0.2: 20%作为测试集
# - random_state=42: 随机种子，保证结果可重复
```

---

## ⚡ PyTorch基础

### 张量（Tensor）
```python
torch.FloatTensor(array)      # 把numpy数组转换成PyTorch张量
tensor.numpy()                # 把张量转换成numpy数组
tensor.shape                  # 张量的形状
```

**什么是张量？**
- 张量就是多维数组，类似于numpy的array
- 但张量可以在GPU上运算，速度更快
- 支持自动求导（自动计算梯度）

### 自动求导
```python
tensor.requires_grad = True   # 启用自动求导
loss.backward()               # 反向传播，计算梯度
optimizer.zero_grad()         # 清零梯度
```

---

## 🧠 神经网络组件

### 基础层
```python
nn.Linear(输入维度, 输出维度)    # 全连接层（线性层）
# 例：nn.Linear(64, 32) 表示输入64维，输出32维
```

**全连接层做什么？**
- 对输入进行线性变换：output = input × weight + bias
- 每个输入都连接到每个输出

### 激活函数
```python
nn.ReLU()                     # ReLU激活函数
# ReLU(x) = max(0, x)  负数变0，正数不变
```

**为什么需要激活函数？**
- 增加非线性，让网络能学习复杂模式
- 没有激活函数，多层网络等同于一层

### 损失函数
```python
nn.MSELoss()                  # 均方误差损失
# MSE = (预测值 - 真实值)² 的平均值
```

### 优化器
```python
torch.optim.Adam(参数, lr=学习率)  # Adam优化器
optimizer.step()               # 更新参数
```

**优化器做什么？**
- 根据梯度更新网络参数
- Adam是一种智能的优化方法，比基础的SGD更好用

---

## 🏋️ 训练相关

### 训练模式切换
```python
model.train()                 # 训练模式（启用dropout、batch norm等）
model.eval()                  # 评估模式（关闭dropout等）
```

### 禁用梯度计算
```python
with torch.no_grad():         # 不计算梯度，节省内存
    predictions = model(X)     # 用于推理/预测时
```

### 模型保存和加载
```python
torch.save(model.state_dict(), '文件名.pth')  # 保存模型参数
model.load_state_dict(torch.load('文件名.pth'))  # 加载模型参数
```

---

## 💡 在代码中的应用

让我们看看这些函数在你的代码中是怎么用的：

### 1. 数据预处理部分
```python
# 替换缺失值
train_data = train_data.replace('-', np.nan)  # 把'-'标记为缺失值
X = np.nan_to_num(X, 0)                      # 把缺失值填充为0

# 标准化
scaler_X = StandardScaler()                   # 创建标准化器
X = scaler_X.fit_transform(X)                # 标准化特征
y = scaler_y.fit_transform(y.reshape(-1, 1)) # 标准化目标值
```

### 2. 模型定义部分
```python
class SimpleModel(nn.Module):
    def __init__(self, input_size):
        super(SimpleModel, self).__init__()
        self.net = nn.Sequential(           # 顺序堆叠层
            nn.Linear(input_size, 64),      # 输入层到隐藏层1
            nn.ReLU(),                      # 激活函数
            nn.Linear(64, 32),              # 隐藏层1到隐藏层2
            nn.ReLU(),                      # 激活函数
            nn.Linear(32, 1)                # 隐藏层2到输出层
        )
```

### 3. 训练循环部分
```python
for epoch in range(EPOCHS):
    model.train()                           # 设为训练模式
    
    pred = model(batch_X)                   # 前向传播
    loss = criterion(pred, batch_y)         # 计算损失
    
    optimizer.zero_grad()                   # 清零梯度
    loss.backward()                         # 反向传播
    optimizer.step()                        # 更新参数
```

### 4. 预测部分
```python
model.eval()                                # 设为评估模式
with torch.no_grad():                       # 不计算梯度
    predictions = model(X_test).numpy()     # 预测并转为numpy

# 反标准化
predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1))
```

---

## 🎯 关键概念总结

1. **标准化**: 让数据分布更均匀，帮助模型更好训练
2. **批处理**: 一次处理多个样本，提高效率
3. **前向传播**: 数据从输入到输出的计算过程
4. **反向传播**: 根据损失计算梯度，用于更新参数
5. **验证集**: 用来评估模型性能，防止过拟合

## 📖 下一步建议

现在你可以：
1. 对照这个文档重新阅读代码
2. 尝试修改一些参数看看效果
3. 添加更多可视化来理解训练过程

如果还有不明白的地方，随时问我！